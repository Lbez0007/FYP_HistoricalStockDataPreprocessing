{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Import Packages"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import CSVs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\Documents\\B.Sc IT (Hons.) Computing and Business\\FYP - Luke Bezzina\\Code\\preprocessingHistoricalData/financial_data\n"
     ]
    }
   ],
   "source": [
    "# Current Path\n",
    "ROOT_DIR = os.path.abspath(os.curdir)\n",
    "path = ROOT_DIR + '/financial_data'\n",
    "print(path)\n",
    "\n",
    "csvfiles = glob.glob(path + \"/*.csv\")\n",
    "# Creating a dataframe for each CSV file\n",
    "dfs = [pd.read_csv(file) for file in csvfiles]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# New Dataframe"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "processed_df_coll = {}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pre-Process"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got scalar array instead:\narray=0.6223375289985335.\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-8-4ee97c20ae51>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     43\u001B[0m         \u001B[0mscalerVol\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mStandardScaler\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     44\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 45\u001B[1;33m         \u001B[0mscalerVol\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mvolatility\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     46\u001B[0m         \u001B[0mscalerTrend\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mslope\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     47\u001B[0m         \u001B[0mvolatility\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mscalerVol\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtransform\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mvolatility\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001B[0m in \u001B[0;36mfit\u001B[1;34m(self, X, y)\u001B[0m\n\u001B[0;32m    665\u001B[0m         \u001B[1;31m# Reset internal state before fitting\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    666\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_reset\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 667\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpartial_fit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mX\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    668\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    669\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mpartial_fit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mX\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001B[0m in \u001B[0;36mpartial_fit\u001B[1;34m(self, X, y)\u001B[0m\n\u001B[0;32m    696\u001B[0m         X = self._validate_data(X, accept_sparse=('csr', 'csc'),\n\u001B[0;32m    697\u001B[0m                                 \u001B[0mestimator\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mFLOAT_DTYPES\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 698\u001B[1;33m                                 force_all_finite='allow-nan')\n\u001B[0m\u001B[0;32m    699\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    700\u001B[0m         \u001B[1;31m# Even in the case of `with_mean=False`, we update the mean anyway\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\base.py\u001B[0m in \u001B[0;36m_validate_data\u001B[1;34m(self, X, y, reset, validate_separately, **check_params)\u001B[0m\n\u001B[0;32m    418\u001B[0m                     \u001B[1;34mf\"requires y to be passed, but the target y is None.\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    419\u001B[0m                 )\n\u001B[1;32m--> 420\u001B[1;33m             \u001B[0mX\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcheck_array\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mX\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mcheck_params\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    421\u001B[0m             \u001B[0mout\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mX\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    422\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\utils\\validation.py\u001B[0m in \u001B[0;36minner_f\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     71\u001B[0m                           FutureWarning)\n\u001B[0;32m     72\u001B[0m         \u001B[0mkwargs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mupdate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m{\u001B[0m\u001B[0mk\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0marg\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mk\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0marg\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mzip\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msig\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mparameters\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0margs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m}\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 73\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     74\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0minner_f\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     75\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\utils\\validation.py\u001B[0m in \u001B[0;36mcheck_array\u001B[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001B[0m\n\u001B[0;32m    615\u001B[0m                     \u001B[1;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    616\u001B[0m                     \u001B[1;34m\"your data has a single feature or array.reshape(1, -1) \"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 617\u001B[1;33m                     \"if it contains a single sample.\".format(array))\n\u001B[0m\u001B[0;32m    618\u001B[0m             \u001B[1;31m# If input is 1D raise error\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    619\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0marray\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mndim\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mValueError\u001B[0m: Expected 2D array, got scalar array instead:\narray=0.6223375289985335.\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "prev_row = None\n",
    "# Up to 0.2% deviation from close price from day before is considered Neutral movement\n",
    "neutral_percentage = 0.002\n",
    "\n",
    "for df in dfs:\n",
    "    processed_df = pd.DataFrame(columns = ['Date','d5_pd', 'd4_pd', 'd3_pd', 'd2_pd', 'd1_pd', 'Trend',\n",
    "                                           'Volatility', 'PreviousClose', 'DayClose'])\n",
    "\n",
    "    # Creating additional feature columns for each dataframe\n",
    "    iterables = df.itertuples(index=True, name='Pandas')\n",
    "    n = df.columns.get_loc('Name') # Name column in df\n",
    "    df_name = df.iat[1, n] # Name of equity/etf\n",
    "\n",
    "    # Per row in dataframe\n",
    "    for i in range(11, len(df) ):\n",
    "        close = df.columns.get_loc('Close')\n",
    "        df_date = df.columns.get_loc('Date')\n",
    "\n",
    "        # Price Trend Handling\n",
    "        close_price = df.iat[i, close]\n",
    "        prev_close_price = df.iat[i - 1, close]\n",
    "        if close_price - prev_close_price > (close_price * neutral_percentage):\n",
    "            direction = 'Positive'\n",
    "        elif close_price - prev_close_price < - (close_price * neutral_percentage):\n",
    "            direction = 'Negative'\n",
    "        else:\n",
    "            direction = 'Neutral'\n",
    "\n",
    "        # Price Difference Handling\n",
    "        d5_pd = df.iat[i - 5, close] - df.iat[0, close]\n",
    "        d4_pd = df.iat[i - 4, close] - df.iat[0, close]\n",
    "        d3_pd = df.iat[i - 3, close] - df.iat[0, close]\n",
    "        d2_pd = df.iat[i - 2, close] - df.iat[0, close]\n",
    "        d1_pd = df.iat[i - 1, close] - df.iat[0, close]\n",
    "        date_ts = df.iat[i,df_date]\n",
    "\n",
    "        pds = [d5_pd, d4_pd, d3_pd, d2_pd, d1_pd]\n",
    "\n",
    "        volatility = np.std(pds)\n",
    "        slope, intercept = np.polyfit([0, 1, 2, 3, 4], pds, 1)\n",
    "\n",
    "        new_row = {'Date':date_ts, 'd5_pd':d5_pd, 'd4_pd':d4_pd, 'd3_pd':d3_pd, 'd2_pd':d2_pd,\n",
    "                   'd1_pd':d1_pd, 'Trend':slope, 'Volatility':volatility, 'PreviousClose':prev_close_price, 'DayClose':close_price}\n",
    "        #append row to the dataframe\n",
    "        processed_df = processed_df.append(new_row, ignore_index=True)\n",
    "\n",
    "    print(processed_df)\n",
    "    processed_df_coll[df_name] = processed_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Export"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for key, df in processed_df_coll.items():\n",
    "    df.to_csv(key+\".csv\", index=False)\n",
    "\n",
    "print(\"Export Complete!\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}