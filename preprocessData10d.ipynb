{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Import Packages"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import CSVs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\FYP - Luke Bezzina\\Code\\preprocessingHistoricalData/financial_data\n"
     ]
    }
   ],
   "source": [
    "# Current Path\n",
    "ROOT_DIR = os.path.abspath(os.curdir)\n",
    "path = ROOT_DIR + '/financial_data'\n",
    "print(path)\n",
    "\n",
    "csvfiles = glob.glob(path + \"/*.csv\")\n",
    "# Creating a dataframe for each CSV file\n",
    "dfs = [pd.read_csv(file) for file in csvfiles]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# New Dataframe"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "processed_df_coll = {}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pre-Process"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fyfy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-4-0c88726d957a>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     68\u001B[0m         \u001B[1;31m# volatility of prices window\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     69\u001B[0m         \u001B[0mvolatility\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstd\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 70\u001B[1;33m         \u001B[0mfyfy\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     71\u001B[0m         \u001B[1;31m# finding trends of close and volume in terms of gradient (slope)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     72\u001B[0m         \u001B[0mslope\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mintercept\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpolyfit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m2\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m3\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m4\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m5\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m6\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m7\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m8\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m9\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpds\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'fyfy' is not defined"
     ]
    }
   ],
   "source": [
    "prev_row = None\n",
    "# Up to 0.2% deviation from close price from day before is considered Neutral movement\n",
    "neutral_percentage = 0.002\n",
    "\n",
    "for df in dfs:\n",
    "    processed_df = pd.DataFrame(columns = ['Date', 'd10_pd', 'd9_pd', 'd8_pd', 'd7_pd', 'd6_pd',\n",
    "                                           'd5_pd', 'd4_pd', 'd3_pd', 'd2_pd', 'd1_pd', 'Trend',\n",
    "                                           'VolumeTrend', 'Volatility', 'PreviousClose', 'PriceDirection'])\n",
    "\n",
    "    # Creating additional feature columns for each dataframe\n",
    "    iterables = df.itertuples(index=True, name='Pandas')\n",
    "    n = df.columns.get_loc('Name') # Name column in df\n",
    "    df_name = df.iat[1, n] # Name of equity/etf\n",
    "\n",
    "    # Iterating all rows in dataframe\n",
    "    # Starting from size n + 1 - prices from past n days used per record\n",
    "    for i in range(11, len(df) ):\n",
    "        close = df.columns.get_loc('Close')\n",
    "        volume = df.columns.get_loc('Volume')\n",
    "        df_date = df.columns.get_loc('Date')\n",
    "\n",
    "        # Price Trend Handling\n",
    "        close_price = df.iat[i, close]\n",
    "        prev_close_price = df.iat[i - 1, close]\n",
    "        price_change = close_price - prev_close_price\n",
    "\n",
    "        if price_change > (close_price * neutral_percentage):\n",
    "            direction = 'Positive'\n",
    "        elif price_change < 0 and abs(price_change) > (close_price * neutral_percentage):\n",
    "            direction = 'Negative'\n",
    "        else:\n",
    "            direction = 'Neutral'\n",
    "\n",
    "        # Price Difference Handling\n",
    "        initial_close = df.iat[i - 11, close]\n",
    "\n",
    "        d10_pd = df.iat[i - 10, close] - initial_close\n",
    "        d9_pd = df.iat[i - 9, close] - initial_close\n",
    "        d8_pd = df.iat[i - 8, close] - initial_close\n",
    "        d7_pd = df.iat[i - 7, close] - initial_close\n",
    "        d6_pd = df.iat[i - 6, close] - initial_close\n",
    "        d5_pd = df.iat[i - 5, close] - initial_close\n",
    "        d4_pd = df.iat[i - 4, close] - initial_close\n",
    "        d3_pd = df.iat[i - 3, close] - initial_close\n",
    "        d2_pd = df.iat[i - 2, close] - initial_close\n",
    "        d1_pd = df.iat[i - 1, close] - initial_close\n",
    "\n",
    "        # Volume Difference Handling\n",
    "        initial_vol = df.iat[i - 11, volume] + 1 # Since volume traded can be 0, handling DIV by 0 issues\n",
    "\n",
    "        vd10_pd = df.iat[i - 10, volume] / initial_vol\n",
    "        vd9_pd = df.iat[i - 9, volume] / initial_vol\n",
    "        vd8_pd = df.iat[i - 8, volume] / initial_vol\n",
    "        vd7_pd = df.iat[i - 7, volume] / initial_vol\n",
    "        vd6_pd = df.iat[i - 6, volume] / initial_vol\n",
    "        vd5_pd = df.iat[i - 5, volume] / initial_vol\n",
    "        vd4_pd = df.iat[i - 4, volume] / initial_vol\n",
    "        vd3_pd = df.iat[i - 3, volume] / initial_vol\n",
    "        vd2_pd = df.iat[i - 2, volume] / initial_vol\n",
    "        vd1_pd = df.iat[i - 1, volume] / initial_vol\n",
    "\n",
    "        date_ts = df.iat[i,df_date]\n",
    "\n",
    "        # 10-day windows for close price and volume\n",
    "        pds = [d1_pd, d2_pd, d3_pd, d4_pd, d5_pd, d6_pd, d7_pd, d8_pd, d9_pd, d10_pd]\n",
    "        vpds = [vd1_pd, vd2_pd, vd3_pd, vd4_pd, vd5_pd, vd6_pd, vd7_pd, vd8_pd, vd9_pd, vd10_pd]\n",
    "\n",
    "        # volatility of prices window\n",
    "        volatility = np.std(pds)\n",
    "\n",
    "        # finding trends of close and volume in terms of gradient (slope)\n",
    "        slope, intercept = np.polyfit([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], pds, 1)\n",
    "        slopeVol, interceptVol = np.polyfit([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], vpds, 1)\n",
    "\n",
    "        new_row = {'Date':date_ts, 'd10_pd':d10_pd, 'd9_pd':d9_pd, 'd8_pd':d8_pd, 'd7_pd':d7_pd,\n",
    "                   'd6_pd':d6_pd, 'd5_pd':d5_pd, 'd4_pd':d4_pd, 'd3_pd':d3_pd, 'd2_pd':d2_pd,\n",
    "                   'd1_pd':d1_pd, 'Trend':slope, 'VolumeTrend':slopeVol, 'Volatility':volatility,\n",
    "                   'PreviousClose':prev_close_price, 'PriceDirection':direction}\n",
    "\n",
    "        #append row to the dataframe\n",
    "        processed_df = processed_df.append(new_row, ignore_index=True)\n",
    "\n",
    "    print(processed_df)\n",
    "    processed_df_coll[df_name] = processed_df\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Export"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for key, df in processed_df_coll.items():\n",
    "    df.to_csv(ROOT_DIR+'\\\\classification_10d\\\\'+key+\".csv\", index=False)\n",
    "\n",
    "print(\"Export Complete!\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}